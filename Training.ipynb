{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "mount_file_id": "1hlE2UK3W2kDPVaIR1ZYRNNMabiYDGpRa",
      "authorship_tag": "ABX9TyNtqWYgYptANIvWcWPKVbzX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cihankaradogan/Stylegan2-Ada-Training/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9T4CapDZYJ"
      },
      "source": [
        "Pull the latest version of stylegan2-ada from github, Use the right version of TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_FVqFnF_ko7",
        "outputId": "70f4d517-c72a-4ec3-db17-78fa12efbbc6"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download the code\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git\n",
        "%cd stylegan2-ada\n",
        "!nvcc test_nvcc.cu -o test_nvcc -run\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Cloning into 'stylegan2-ada'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n",
            "/content/stylegan2-ada\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Ktest_nvcc.cu: No such file or directory\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K-x c++\u001b[m\u001b[K’ after last input file has no effect\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n",
            "Tensorflow version: 1.15.2\n",
            "GPU 0: Tesla K80 (UUID: GPU-5cd82b9f-5935-4381-4245-6898f03284fa)\n",
            "GPU Identified at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfuoo54Dhsx"
      },
      "source": [
        "# Prepare the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7-AXPh1D0ae"
      },
      "source": [
        "The below cell creates a TF records file which stylegan2 ada needs to train successfully!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3XWdxxyDift",
        "outputId": "44e75676-e7f7-4b77-d209-befe5fd21d07"
      },
      "source": [
        "%cd /content/stylegan2-ada/\n",
        "!python dataset_tool.py create_from_images /content/datasets/custom/ /content/drive/MyDrive/samples/\n",
        "                                            #out dir                    data's dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada\n",
            "Loading images from \"/content/drive/MyDrive/samples/\"\n",
            "Creating dataset \"/content/datasets/custom/\"\n",
            "dataset_tool.py:96: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 101 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lk2Ura-Dms2",
        "outputId": "b78eeadf-d328-4779-b29e-3ec3aae4ab60"
      },
      "source": [
        "%cd /content/stylegan2-ada"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHGMF8EEVyb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ossHBREYD6",
        "outputId": "936d345c-c065-45dd-9865-fe52f8af08fb"
      },
      "source": [
        "!python train.py --outdir=/training-runs --gpus=1 --data=/content/datasets/custom \\\n",
        "      --cfg=paper1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55b8f031e000 @  0x7f03a7170001 0x7f03a43b354f 0x7f03a4403b58 0x7f03a4407b17 0x7f03a44a6203 0x55b8e8e3c0a4 0x55b8e8e3bda0 0x55b8e8eb0868 0x55b8e8eab235 0x55b8e8e3dfec 0x55b8e8e7ebc9 0x55b8e8e7bac4 0x55b8e8e3c8a9 0x55b8e8eb0b0a 0x55b8e8eaac35 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8eab235 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8e3d65a 0x55b8e8eabb0e 0x55b8e8eaac35 0x55b8e8eaa933 0x55b8e8f74402 0x55b8e8f7477d 0x55b8e8f74626 0x55b8e8f4c313\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b9f031e000 @  0x7f03a716e1e7 0x7f03a43b346e 0x7f03a4403c7b 0x7f03a440435f 0x7f03a44a6103 0x55b8e8e3c0a4 0x55b8e8e3bda0 0x55b8e8eb0868 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8e3d65a 0x55b8e8eabb0e 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eaff40 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eab235 0x55b8e8e3dfec 0x55b8e8e7ebc9 0x55b8e8e7bac4 0x55b8e8e3c8a9 0x55b8e8eb0b0a\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55baf16e8000 @  0x7f03a716e1e7 0x7f03a43b346e 0x7f03a4403c7b 0x7f03a440435f 0x7f0360d75235 0x7f03606f8792 0x7f03606f8d42 0x7f03606b1aee 0x55b8e8e3bf97 0x55b8e8e3bda0 0x55b8e8eb02f9 0x55b8e8e3d65a 0x55b8e8eabd67 0x55b8e8eaadcc 0x55b8e8d7ceb1 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eabd67 0x55b8e8eab235 0x55b8e8e3d73a 0x55b8e8eabd67 0x55b8e8e3d65a 0x55b8e8eabd67 0x55b8e8eaac35 0x55b8e8e3ddd1 0x55b8e8e3e1f1 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eabb0e\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 8,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 2\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1,\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1\n",
            "    }\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 50,\n",
            "  \"network_snapshot_ticks\": 50,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"/content/datasets/custom\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [\n",
            "    {\n",
            "      \"name\": \"fid50k_full\",\n",
            "      \"class_name\": \"metrics.frechet_inception_distance.FID\",\n",
            "      \"max_reals\": null,\n",
            "      \"num_fakes\": 50000,\n",
            "      \"minibatch_per_gpu\": 8,\n",
            "      \"force_dataset_args\": {\n",
            "        \"shuffle\": false,\n",
            "        \"max_images\": null,\n",
            "        \"repeat\": false,\n",
            "        \"mirror_augment\": false\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"/content/datasets/custom\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": false\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"minibatch_size\": 32,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 10,\n",
            "  \"G_smoothing_rampup\": null,\n",
            "  \"run_dir\": \"/training-runs/00001-custom-paper1024\"\n",
            "}\n",
            "\n",
            "Output directory:  /training-runs/00001-custom-paper1024\n",
            "Training data:     /content/datasets/custom\n",
            "Training length:   25000 kimg\n",
            "Resolution:        1024\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b8f001c000 @  0x7f03a7170001 0x7f03a43b354f 0x7f03a4403b58 0x7f03a4407b17 0x7f03a44a6203 0x55b8e8e3c0a4 0x55b8e8e3bda0 0x55b8e8eb0868 0x55b8e8eab235 0x55b8e8e3dfec 0x55b8e8e7ebc9 0x55b8e8e7bac4 0x55b8e8e3c8a9 0x55b8e8eb0b0a 0x55b8e8eaac35 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8eab235 0x55b8e8d7ce2c 0x55b8e8ead318 0x55b8e8e3d65a 0x55b8e8eabb0e 0x55b8e8eaac35 0x55b8e8eaa933 0x55b8e8f74402 0x55b8e8f7477d 0x55b8e8f74626 0x55b8e8f4c313\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55bbf16e8000 @  0x7f03a716e1e7 0x7f03a43b346e 0x7f03a4403c7b 0x7f03a440435f 0x7f03a44a6103 0x55b8e8e3c0a4 0x55b8e8e3bda0 0x55b8e8eb0868 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8e3d65a 0x55b8e8eabb0e 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eaff40 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eac93b 0x55b8e8eab235 0x55b8e8e3dfec 0x55b8e8e7ebc9 0x55b8e8e7bac4 0x55b8e8e3c8a9 0x55b8e8eb0b0a\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55bbf16e8000 @  0x7f03a716e1e7 0x7f03a43b346e 0x7f03a4403c7b 0x7f03a440435f 0x7f0360d75235 0x7f03606f8792 0x7f03606f8d42 0x7f03606b1aee 0x55b8e8e3bf97 0x55b8e8e3bda0 0x55b8e8eb02f9 0x55b8e8e3d65a 0x55b8e8eabd67 0x55b8e8eaadcc 0x55b8e8d7ceb1 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eabd67 0x55b8e8eab235 0x55b8e8e3d73a 0x55b8e8eabd67 0x55b8e8e3d65a 0x55b8e8eabd67 0x55b8e8eaac35 0x55b8e8e3ddd1 0x55b8e8e3e1f1 0x55b8e8ead318 0x55b8e8eaac35 0x55b8e8e3d73a 0x55b8e8eabb0e\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "\n",
            "G                               Params    OutputShape          WeightShape     \n",
            "---                             ---       ---                  ---             \n",
            "latents_in                      -         (?, 512)             -               \n",
            "labels_in                       -         (?, 0)               -               \n",
            "G_mapping/Normalize             -         (?, 512)             -               \n",
            "G_mapping/Dense0                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense1                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense2                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense3                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense4                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense5                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense6                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense7                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Broadcast             -         (?, 18, 512)         -               \n",
            "dlatent_avg                     -         (512,)               -               \n",
            "Truncation/Lerp                 -         (?, 18, 512)         -               \n",
            "G_synthesis/4x4/Const           8192      (?, 512, 4, 4)       (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv            2622465   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB           264195    (?, 3, 4, 4)         (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up        2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1           2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample        -         (?, 3, 8, 8)         -               \n",
            "G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               \n",
            "G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               \n",
            "G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               \n",
            "G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up    1442561   (?, 256, 128, 128)   (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1       721409    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               \n",
            "G_synthesis/128x128/ToRGB       132099    (?, 3, 128, 128)     (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up    426369    (?, 128, 256, 256)   (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1       213249    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               \n",
            "G_synthesis/256x256/ToRGB       66051     (?, 3, 256, 256)     (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up    139457    (?, 64, 512, 512)    (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1       69761     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               \n",
            "G_synthesis/512x512/ToRGB       33027     (?, 3, 512, 512)     (1, 1, 64, 3)   \n",
            "G_synthesis/1024x1024/Conv0_up  51297     (?, 32, 1024, 1024)  (3, 3, 64, 32)  \n",
            "G_synthesis/1024x1024/Conv1     25665     (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/1024x1024/ToRGB     16515     (?, 3, 1024, 1024)   (1, 1, 32, 3)   \n",
            "---                             ---       ---                  ---             \n",
            "Total                           30370060                                       \n",
            "\n",
            "\n",
            "D                     Params    OutputShape          WeightShape     \n",
            "---                   ---       ---                  ---             \n",
            "images_in             -         (?, 3, 1024, 1024)   -               \n",
            "labels_in             -         (?, 0)               -               \n",
            "1024x1024/FromRGB     128       (?, 32, 1024, 1024)  (1, 1, 3, 32)   \n",
            "1024x1024/Conv0       9248      (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "1024x1024/Conv1_down  18496     (?, 64, 512, 512)    (3, 3, 32, 64)  \n",
            "1024x1024/Skip        2048      (?, 64, 512, 512)    (1, 1, 32, 64)  \n",
            "512x512/Conv0         36928     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "512x512/Conv1_down    73856     (?, 128, 256, 256)   (3, 3, 64, 128) \n",
            "512x512/Skip          8192      (?, 128, 256, 256)   (1, 1, 64, 128) \n",
            "256x256/Conv0         147584    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "256x256/Conv1_down    295168    (?, 256, 128, 128)   (3, 3, 128, 256)\n",
            "256x256/Skip          32768     (?, 256, 128, 128)   (1, 1, 128, 256)\n",
            "128x128/Conv0         590080    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "128x128/Conv1_down    1180160   (?, 512, 64, 64)     (3, 3, 256, 512)\n",
            "128x128/Skip          131072    (?, 512, 64, 64)     (1, 1, 256, 512)\n",
            "64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)\n",
            "32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)\n",
            "16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)\n",
            "8x8/Conv0             2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "8x8/Conv1_down        2359808   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "8x8/Skip              262144    (?, 512, 4, 4)       (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev   -         (?, 513, 4, 4)       -               \n",
            "4x4/Conv              2364416   (?, 512, 4, 4)       (3, 3, 513, 512)\n",
            "4x4/Dense0            4194816   (?, 512)             (8192, 512)     \n",
            "Output                513       (?, 1)               (512, 1)        \n",
            "---                   ---       ---                  ---             \n",
            "Total                 29012513                                       \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 25000 kimg...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJw-Z2hcEf11"
      },
      "source": [
        "  --outdir DIR          Where to save the results (required)\n",
        "\n",
        "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
        "\n",
        "  --data PATH           Training dataset path (required)\n",
        "\n",
        "  --kimg INT            Override training duration\n",
        "\n",
        "**configs (--cfg):**\n",
        "\n",
        "  auto           Automatically select reasonable defaults based on resolution\n",
        "                 and GPU count. Good starting point for new datasets.\n",
        "\n",
        "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
        "\n",
        "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
        "\n",
        "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
        "\n",
        "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
        "\n",
        "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
        "\n",
        "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
        "\n",
        "\n",
        "**transfer learning source networks (--resume):**\n",
        "\n",
        "  ffhq256        FFHQ trained at 256x256 resolution.\n",
        "\n",
        "  ffhq512        FFHQ trained at 512x512 resolution.\n",
        "\n",
        "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
        "\n",
        "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
        "\n",
        "  lsundog256     LSUN Dog trained at 256x256 resolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn93zhBoFqXm"
      },
      "source": [
        "# Convert it to a bigger model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ9ZFY91E1EK"
      },
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/aydao/stylegan2-surgery.git\n",
        "%cd stylegan2-surgery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r-8IfqSGDnM"
      },
      "source": [
        "The following will create a new, untrained network with your specified resolution. We need this to copy weights into.\n",
        "\n",
        "Change the **width** and **height** values!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV2nZPT8F716"
      },
      "source": [
        "!python create_initial_network_pkl.py --width 4096 --height 4096"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPFDY7q8GITv"
      },
      "source": [
        "### Copying Weights\n",
        "Now that we have an empty network we need to copy weights into it.\n",
        "\n",
        "What you need to do here if you want to use this network is change the height and width in the final argument of the following command. They're in bold below...\n",
        "\n",
        "network-initial-config-f-4096x4096-0.pkl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5iR5l7zGKuQ"
      },
      "source": [
        "!python copy_weights.py /training-runs/stylegan2-ffhq-config-f.pkl /content/stylegan2-surgery/network-initial-config-f-4096x4096-0.pkl --output_pkl /content/surgery_output.pkl\n",
        "                            #the previous model we trained         #model with 4096 resolution that we created in the previous code              #the model we will get as output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9SKy5ujG9_P"
      },
      "source": [
        "# Generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_X0GtqNHBV4"
      },
      "source": [
        "!python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n",
        "    --network=/content/surgery_output.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry6CBt-DJwiu"
      },
      "source": [
        "  --outdir DIR          Where to save the results (required)\n",
        "\n",
        "  --seeds INT           Random number range (600-605 generates 6 images)  \n",
        "\n",
        "  --network PATH        Path of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHsRUqEeJw0e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}